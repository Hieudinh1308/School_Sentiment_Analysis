


import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
  print('Found GPU at: {}'.format(device_name))


!pip install -q transformers


!pip install datasets


import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertConfig
from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
import pandas as pd
import io
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
n_gpu = torch.cuda.device_count()
torch.cuda.get_device_name(0)





from datasets import load_dataset,DatasetDict

dataset = load_dataset("uit-nlp/vietnamese_students_feedback")


dataset


for i in range(100):
  print(dataset['train']['sentence'][i], "lable :",dataset['train']['sentiment'][i])


# Remove the 'topic' column
filtered_train_dataset = dataset["train"].remove_columns("topic")
filtered_validation_dataset = dataset["validation"].remove_columns("topic")
filtered_test_dataset = dataset["test"].remove_columns("topic")

# Create a new DatasetDict with filtered datasets
filtered_dataset_dict = DatasetDict({
    "train": filtered_train_dataset,
    "validation": filtered_validation_dataset,
    "test": filtered_test_dataset,
})


filtered_dataset_dict


for i in range(10):
  print(filtered_dataset_dict['train']['sentence'][i], "lable :",filtered_dataset_dict['train']['sentiment'][i])


from transformers import RobertaForSequenceClassification, AutoTokenizer , AdamW
model = RobertaForSequenceClassification.from_pretrained("wonrax/phobert-base-vietnamese-sentiment")

tokenizerss = AutoTokenizer.from_pretrained("wonrax/phobert-base-vietnamese-sentiment", use_fast=False)
model.to(device)


def tokenize_batch(batch):
    return tokenizerss(batch['sentence'], padding='max_length', truncation=True,max_length=128)

tokenizer = filtered_dataset_dict.map(tokenize_batch, batched=True)


num_labels = 3
batch_size = 32
learning_rate = 2e-5
num_epochs = 6


from torch.utils.data import DataLoader

# Create data loaders with dynamic padding
train_dataloader = DataLoader(
    tokenizer['train'],
    batch_size=batch_size,
    shuffle=True,
    collate_fn=tokenizer['train'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'sentiment']),
)
eval_dataloader = DataLoader(
    tokenizer['test'],
    batch_size=batch_size,
    collate_fn=tokenizer['test'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'sentiment']),
)


# Define optimizer and loss function
optimizer = AdamW(model.parameters(), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()


# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    for batch in train_dataloader:
        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['sentiment'].to(device)
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_train_loss = total_loss / len(train_dataloader)
    print(f"Epoch {epoch + 1}/{num_epochs} - Average training loss: {avg_train_loss:.4f}")


# Evaluation loop
model.eval()
correct_predictions = 0
total_predictions = 0

with torch.no_grad():
    for batch in eval_dataloader:
        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['sentiment'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predicted_labels = torch.argmax(logits, dim=1)
        correct_predictions += (predicted_labels == labels).sum().item()
        total_predictions += labels.size(0)

accuracy = correct_predictions / total_predictions
print(f"Accuracy on the evaluation set: {accuracy:.4f}")


# Save the fine-tuned model
model.save_pretrained('fine_tuned_phobert_model_vn_pt')


!huggingface-cli login


!pip install huggingface_hub


tokenizerss.push_to_hub("BERT_Sentiment_Vietnamese")


model.push_to_hub("BERT_Sentiment_Vietnamese")



